
正答率80%点超えるのが難しい

transformerモデルでは、30秒の学習で50~60点まで持っていけるが、30分かけた学習でも50~60点以上成長をしない。
また過学習を起こしやすい。

いただいた知見：
・90点以上を目指すにはlearning_rateを0.0001にする
・1000epochを行う
・タスクがシンプルな場合はモデルのパラメーター数や構造もシンプルにするべき。①複雑だと学習の収束が遅くなる ②過学習のリスク
(例：レイヤー数、ユニット数を減らす)

→ モデルのサイズをできる限り小さくして、計算量を削減し、その分エポック数を多くすることで学習を進めるというアドバイス


gruで3000データセットで1000epochsを回そうと思ったが、早期停止してしまった。
その時のパラメーター↓
        "batch_size": 64,
        "learning_rate": 0.0001,
        "embedding_dim": 32,
        "gru_units": 64,
        "dropout_rate": 0.2,
        "recurrent_dropout_rate": 0.2,
        "epochs": 1000

    
つまり早期停止を防ぐ方法を考える必要がある。そのために次のパラメーターで回してみた

⭕️アプローチ1；モデルサイズを小さくし、学習率を下げ、エポック数を増やす 方向性での検証(検証中)
モデルを小さくし、エポック数を増やし、学習率を下げることで性能を向上させる

モデルのサイズ: 小さくする（embedding_dimやgru_unitsを減らす）。
過学習対策: モデルのサイズを小さくして、過学習を防ぐ。
エポック数: 増やす（例：1000以上）。
学習率: 下げる（例：0.0001に設定）。
理論: 小さなモデルは過学習しにくく、エポック数を多く回すことでモデルがデータからより多くの情報を学習する。学習率を下げることで、より安定して学習が進む。

"batch_size": 32, ←バッチサイズを減らし詳細な学習を可能にする
"learning_rate": 0.00001, ←学習率をさらに下げる
"embedding_dim": 16, ←複雑さを減らす
"gru_units": 32, ←複雑さを減らす
"dropout_rate": 0.3, ←dropout_rateをあげて過学習を防止する
"recurrent_dropout_rate": 0.3, ←dropout_rateをあげて過学習を防止する
"epochs": 1000  # ただし、早期停止により適切なエポック数で止まる


⭕️アプローチ2 :モデルの複雑度を上げ、データセットを大幅に増やす
大きなモデルを使用し、データセットを大幅に増やすことで性能を向上させる

データセット数 100,000
"batch_size": 64,
"learning_rate": 0.0001,
"embedding_dim": 128,
"gru_units": 256,
"dropout_rate": 0.2,
"recurrent_dropout_rate": 0.2,
"epochs": 1000  # 早期停止を有効にする
大きなモデルを使用し、データセットを大幅に増やすことで性能を向上させる

モデルのサイズ: 大きくする（embedding_dimやgru_unitsを増やす）。
過学習対策: データセットの数を大幅に増やす（例：3000から30万へ）。
理論: 大きなモデルは複雑なパターンを学習する能力が高いが、過学習しやすい。データセットを増やすことで、モデルがより多くのデータを学習し、汎化性能を向上させる。