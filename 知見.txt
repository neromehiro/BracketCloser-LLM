
正答率80%点超えるのが難しい

transformerモデルでは、30秒の学習で50~60点まで持っていけるが、30分かけた学習でも50~60点以上成長をしない。
また過学習を起こしやすい。

いただいた知見：
・90点以上を目指すにはlearning_rateを0.0001にする
・1000epochを行う
・タスクがシンプルな場合はモデルのパラメーター数や構造もシンプルにするべき。①複雑だと学習の収束が遅くなる ②過学習のリスク
(例：レイヤー数、ユニット数を減らす)

→ モデルのサイズをできる限り小さくして、計算量を削減し、その分エポック数を多くすることで学習を進めるというアドバイス


gruで3000データセットで1000epochsを回そうと思ったが、早期停止してしまった。
その時のパラメーター↓
        "batch_size": 64,
        "learning_rate": 0.0001,
        "embedding_dim": 32,
        "gru_units": 64,
        "dropout_rate": 0.2,
        "recurrent_dropout_rate": 0.2,
        "epochs": 1000

    
つまり早期停止を防ぐ方法を考える必要がある。そのために次のパラメーターで回してみた


"batch_size": 32, ←バッチサイズを減らし詳細な学習を可能にする
"learning_rate": 0.00001, ←学習率をさらに下げる
"embedding_dim": 16, ←複雑さを減らす
"gru_units": 32, ←複雑さを減らす
"dropout_rate": 0.3, ←dropout_rateをあげて過学習を防止する
"recurrent_dropout_rate": 0.3, ←dropout_rateをあげて過学習を防止する
"epochs": 1000  # ただし、早期停止により適切なエポック数で止まる
