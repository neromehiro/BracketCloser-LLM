
# **プロジェクトで得られた知見**


## 得られた知見
* 1分の学習で40~50パーセントの正答率は容易に達成できる(loss 0.7程度)
* 30minの学習で50~70%も達成可能(loss 0.6程度)
* 2hour程度で65~75%も達成可能(loss 0.5前後)


### **だが75~95%以上がむずかしい**

## 課題
* 目標である95パーセント以上目指すのが難しい
  * lossが0.5よりも下がらず過学習を起こしてしまう.
  → 65~75%以上、モデルの性能が上がらない問題

## 対策

* ## アプローチ A (モデルサイズを小さくし、学習率を下げ、エポック数を増やす)

        モデルを小さくし、エポック数を増やし、学習率を下げることで性能を向上させる

* モデルのサイズ: 小さくする（embedding_dimやgru_unitsを減らす）。
* 過学習対策: モデルのサイズを小さくして、過学習を防ぐ。
* エポック数: 増やす（例：1000以上）。
* 学習率: 下げる（例：0.0001に設定）。
* 理論: 小さなモデルは過学習しにくく、エポック数を多く回すことでモデルがデータからより多くの情報を学習する。学習率を下げることで、より安定して学習が進む。

* 90点以上を目指すにはlearning_rateを0.0001にする
* 1000epochを行う
* タスクがシンプルな場合はモデルのパラメーター数や構造もシンプルにするべき。①複雑だと学習の収束が遅くなる ②過学習のリスク
(例：レイヤー数、ユニット数を減らす)

        gruで3000データセットで1000epochsを回そうと思ったが、140epochで早期停止してしまった。
その時のパラメーター
```markdown
        データセットサイズ 3000個 
        モデル gru
        "batch_size": 64,
        "learning_rate": 0.0001,
        "embedding_dim": 32,
        "gru_units": 64,
        "dropout_rate": 0.2,
        "recurrent_dropout_rate": 0.2,
        "epochs": 1000(実際は140epochでearly stop)
``` 
---
議論
* 過学習を起こしてもスルーし、epochs数を1000まで回すとどのような効果が出るのか
* モデルを小さくして、epochs数を多く回すと、モデルの表現力は足りるのか。
* この場合、データセットの数はどれくらい必要か


---

* ## アプローチ B (モデルの複雑度を上げ、データセットを大幅に増やす)

大きなモデルを使用し、データセットを大幅に増やすことで性能を向上させる
* モデルのサイズ: 大きくする（embedding_dimやgru_unitsを増やす）。
* 過学習対策: データセットの数を大幅に増やす（例：3000から30万へ）。
* 理論: 大きなモデルは複雑なパターンを学習する能力が高いが、過学習しやすい。データセットを増やすことで、モデルがより多くのデータを学習し、汎化性能を向上させる。

```markdown
データセット数 100,000
"batch_size": 64,
"learning_rate": 0.0001,
"embedding_dim": 128,
"gru_units": 256,
"dropout_rate": 0.2,
"recurrent_dropout_rate": 0.2,
"epochs": 1000  # 早期停止を有効にする
```
大きなモデルを使用し、データセットを大幅に増やすことで性能を向上させる

モデルのサイズ: 大きくする（embedding_dimやgru_unitsを増やす）。

* 過学習対策: データセットの数を大幅に増やす（例：3000から30万へ）。
* 理論: 大きなモデルは複雑なパターンを学習する能力が高いが、過学習しやすい。データセットを増やすことで、モデルがより多くのデータを学習し、汎化性能を向上させる。
---

### 学習が最適にいく場合の要素の分解
* モデルの表現力が足りている(知能が足りている?)
* パターン分解
  * 賢い×大量のデータセット×小エポック
  * 賢くない×大量のエポック(過学習を起こしても無視.起こさないように注意)
* Lossの低下の値と、モデルの正答率が相関があるかを研究する


## 解決手段

## ネクストアクション

## メモ
0から精度の高いモデルを作る方法
1. データセットの複雑さを把握し、モデルが過学習を起こさず表現力も不足しない最適なパラメーターを見つける
2. 大量のデータセット×小エポックで学習を開始
3. 少量のデータセット×大エポックで学習を開始(モデルの複雑度を下げる)




# 殴り書き

transformerモデルでは、30秒の学習で50~60点まで持っていけるが、30分かけた学習でも50~60点以上成長をしない。
また過学習を起こしやすい。

いただいた知見：
⭕️アプローチ1；モデルサイズを小さくし、学習率を下げ、エポック数を増やす 方向性での検証(検証中)
モデルを小さくし、エポック数を増やし、学習率を下げることで性能を向上させる

・90点以上を目指すにはlearning_rateを0.0001にする
・1000epochを行う
・タスクがシンプルな場合はモデルのパラメーター数や構造もシンプルにするべき。①複雑だと学習の収束が遅くなる ②過学習のリスク
(例：レイヤー数、ユニット数を減らす)


gruで3000データセットで1000epochsを回そうと思ったが、早期停止してしまった。
その時のパラメーター↓
        データセットサイズ 3000個
        "batch_size": 64,
        "learning_rate": 0.0001,
        "embedding_dim": 32,
        "gru_units": 64,
        "dropout_rate": 0.2,
        "recurrent_dropout_rate": 0.2,
        "epochs": 1000(実際は140epochでearly stop)

    
つまり早期停止を防ぐ方法を考える必要がある。そのために次のパラメーターで回してみた

モデルのサイズ: 小さくする（embedding_dimやgru_unitsを減らす）。
過学習対策: モデルのサイズを小さくして、過学習を防ぐ。
エポック数: 増やす（例：1000以上）。
学習率: 下げる（例：0.0001に設定）。
理論: 小さなモデルは過学習しにくく、エポック数を多く回すことでモデルがデータからより多くの情報を学習する。学習率を下げることで、より安定して学習が進む。

"batch_size": 32, ←バッチサイズを減らし詳細な学習を可能にする
"learning_rate": 0.00001, ←学習率をさらに下げる
"embedding_dim": 16, ←複雑さを減らす
"gru_units": 32, ←複雑さを減らす
"dropout_rate": 0.3, ←dropout_rateをあげて過学習を防止する
"recurrent_dropout_rate": 0.3, ←dropout_rateをあげて過学習を防止する
"epochs": 1000  # ただし、早期停止により適切なエポック数で止まる

過学習によって停止してしまった。アプローチ1で過学習が起きても問題ないようにearly stopを停止して、1000epochを回し、数値を確かめる必要がある。

⭕️アプローチ2 :モデルの複雑度を上げ、データセットを大幅に増やす
大きなモデルを使用し、データセットを大幅に増やすことで性能を向上させる

データセット数 100,000
"batch_size": 64,
"learning_rate": 0.0001,
"embedding_dim": 128,
"gru_units": 256,
"dropout_rate": 0.2,
"recurrent_dropout_rate": 0.2,
"epochs": 1000  # 早期停止を有効にする
大きなモデルを使用し、データセットを大幅に増やすことで性能を向上させる

モデルのサイズ: 大きくする（embedding_dimやgru_unitsを増やす）。
過学習対策: データセットの数を大幅に増やす（例：3000から30万へ）。
理論: 大きなモデルは複雑なパターンを学習する能力が高いが、過学習しやすい。データセットを増やすことで、モデルがより多くのデータを学習し、汎化性能を向上させる。

タスク

過学習によって停止してしまった。アプローチ1で過学習が起きても問題ないようにearly stopを停止して、1000epochを回し、数値を確かめる必要がある。